import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import init


# from .data import batch_generator
# from utils import extract_time, random_generator, NormMinMax


def _weights_init(m):
    classname = m.__class__.__name__
    if isinstance(m, nn.Linear):
        init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0)
    elif classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('Norm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)
    elif classname.find("GRU") != -1:
        for name, param in m.named_parameters():
            if 'weight_ih' in name:
                init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)


class Encoder(nn.Module):
    """Embedding network between original feature space to latent space.

        Args:
          - input: input time-series features. (L, N, X) = (24, ?, 6)
          - h3: (num_layers, N, H). [3, ?, 24]

        Returns:
          - H: embeddings
        """

    def __init__(self, opt):
        super(Encoder, self).__init__()
        self.rnn = nn.GRU(input_size=opt.in_dim, hidden_size=opt.embed_dim, num_layers=opt.num_layer, batch_first=True)
        # self.norm = nn.BatchNorm1d(opt.hidden_dim)
        self.fc = nn.Linear(opt.embed_dim, opt.embed_dim)
        self.tanh = nn.Tanh()
        self.apply(_weights_init)

    def forward(self, input, tanh=True):
        e_outputs, _ = self.rnn(input)
        H = self.fc(e_outputs)
        if tanh:
            H = self.tanh(H)
        return H


class Recovery(nn.Module):
    """Recovery network from latent space to original space.

    Args:
      - H: latent representation
      - T: input time information

    Returns:
      - X_tilde: recovered data
    """

    def __init__(self, opt):
        super(Recovery, self).__init__()
        self.rnn = nn.GRU(input_size=opt.embed_dim, hidden_size=opt.in_dim, num_layers=opt.num_layer, batch_first=True)

        #  self.norm = nn.BatchNorm1d(opt.in_dim)
        self.fc = nn.Linear(opt.in_dim, opt.in_dim)
        self.tanh = nn.Tanh()
        self.apply(_weights_init)

    def forward(self, input, tanh=False):
        r_outputs, _ = self.rnn(input)
        X_tilde = self.fc(r_outputs)
        if tanh:
            X_tilde = self.tanh(X_tilde)
        return X_tilde


class Generator(nn.Module):
    """Generator function: Generate time-series data in latent space.

    Args:
      - Z: random variables
      - T: input time information

    Returns:
      - E: generated embedding
    """

    def __init__(self, opt):
        super(Generator, self).__init__()
        self.rnn = nn.GRU(input_size=opt.cond_dim + opt.embed_dim, hidden_size=opt.embed_dim, num_layers=opt.num_layer, batch_first=True)
        #   self.norm = nn.LayerNorm(opt.hidden_dim)
        self.fc = nn.Linear(opt.embed_dim, opt.embed_dim)
        self.tanh = nn.Tanh()
        self.apply(_weights_init)

    def forward(self, input, conditional_feats, tanh=True):
        input_ = torch.cat((input, conditional_feats), dim=2)
        g_outputs, _ = self.rnn(input_)
        #  g_outputs = self.norm(g_outputs)
        E = self.fc(g_outputs)
        if tanh:
            E = self.tanh(E)
        return E


class Supervisor(nn.Module):
    """Generate next sequence using the previous sequence.

    Args:
      - H: latent representation
      - T: input time information

    Returns:
      - S: generated sequence based on the latent representations generated by the generator
    """

    def __init__(self, opt):
        super(Supervisor, self).__init__()
        self.rnn = nn.GRU(input_size=opt.cond_dim + opt.embed_dim, hidden_size=opt.embed_dim, num_layers=opt.num_layer, batch_first=True)
        #  self.norm = nn.LayerNorm(opt.hidden_dim)
        self.fc = nn.Linear(opt.embed_dim, opt.embed_dim)
        self.tanh = nn.Tanh()
        self.apply(_weights_init)

    def forward(self, input, conditional_feats, tanh=True):
        input_ = torch.cat((input, conditional_feats), dim=2)
        s_outputs, _ = self.rnn(input_)
        #  s_outputs = self.norm(s_outputs)
        S = self.fc(s_outputs)
        if tanh:
            S = self.tanh(S)
        return S


class Discriminator(nn.Module):
    """Discriminate the original and synthetic time-series data.

    Args:
      - H: latent representation
      - T: input time information

    Returns:
      - Y_hat: classification results between original and synthetic time-series
    """

    def __init__(self, opt):
        super(Discriminator, self).__init__()
        self.rnn = nn.GRU(input_size=opt.embed_dim, hidden_size=opt.embed_dim, num_layers=opt.num_layer)
        #  self.norm = nn.LayerNorm(opt.hidden_dim)
        self.fc = nn.Linear(opt.embed_dim, opt.embed_dim)
        self.sigmoid = nn.Sigmoid()
        self.apply(_weights_init)

    def forward(self, input, sigmoid=True):
        d_outputs, _ = self.rnn(input)
        Y_hat = self.fc(d_outputs)
        if sigmoid:
            Y_hat = self.sigmoid(Y_hat)
        return Y_hat


class TimeGAN(nn.Module):
    """TimeGAN Class"""
    def __init__(self, opt):
        super(TimeGAN, self).__init__()
        self.opt = opt
        self.nete = Encoder(self.opt)
        self.netr = Recovery(self.opt)
        self.netg = Generator(self.opt)
        self.netd = Discriminator(self.opt)
        self.nets = Supervisor(self.opt)

        # loss
        self.l_mse = nn.MSELoss()
        self.l_r = nn.L1Loss()
        self.l_bce = nn.BCELoss()

        # Setup optimizer
        # if self.opt.isTrain:
        self.nete.train()
        self.netr.train()
        self.netg.train()
        self.netd.train()
        self.nets.train()
        self.optimizer_e = optim.Adam(self.nete.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999))
        self.optimizer_r = optim.Adam(self.netr.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999))
        self.optimizer_g = optim.Adam(self.netg.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999))
        self.optimizer_d = optim.Adam(self.netd.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999))
        self.optimizer_s = optim.Adam(self.nets.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999))
